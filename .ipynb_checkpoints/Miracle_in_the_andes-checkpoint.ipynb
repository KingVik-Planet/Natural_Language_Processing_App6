{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b61c09-ed67-433d-a3e2-97c16e129e51",
   "metadata": {},
   "source": [
    "# Load the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33ad62a9-0d7b-457f-b421-5c316891a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"miracle_in_the_andes.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    book = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0404bdc0-c268-4e16-9ebe-9f64807f67da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde00d9-5377-4923-98db-953996a7408b",
   "metadata": {},
   "source": [
    "# Numbers of Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b449646-0663-43c3-aac6-6b6b05c1c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.count(\"Chapter\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2151d8-f4ef-4fe6-a65d-81ad3d3e151a",
   "metadata": {},
   "source": [
    "# With Regular Expression (regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f38e38-7ba1-4a8a-a404-2a9003002905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67efd381-694a-4a1e-8c15-81992e5086d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter', 'before', 'it', 'was', 'friday']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(\"[a-zA-Z]+\")\n",
    "findings = re.findall(pattern, book.lower())\n",
    "findings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20f05f-f6cc-40e6-a93b-15c91cbba6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d  = {}\n",
    "for word in findings:\n",
    "    if word in d.keys():\n",
    "        d[word] = d[word] + 1\n",
    "    else:\n",
    "        d[word] = 1\n",
    "        \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7950356-efd6-464b-831a-fe63e7bb2a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1430, 'was'), (800, 'it'), (93, 'before'), (11, 'chapter'), (1, 'friday')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_list = [(value, key) for (key, value) in d.items()]\n",
    "sorted(d_list[:5], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1fb15-8a9e-4577-a2a5-5c3c60d76e4f",
   "metadata": {},
   "source": [
    "# Checking python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9aa39d0-6e15-43f0-8148-7ae6abbf1f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02959d4-912c-4fde-9219-631e8e2ddfcd",
   "metadata": {},
   "source": [
    "# Installing nltk libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "561f8298-8cdc-4042-8ded-9b39ca0f65ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\kingvik planet\\pycharmprojects\\newsapi\\natural_language_processing_app6\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\kingvik planet\\pycharmprojects\\newsapi\\natural_language_processing_app6\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kingvik planet\\pycharmprojects\\newsapi\\natural_language_processing_app6\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kingvik planet\\pycharmprojects\\newsapi\\natural_language_processing_app6\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kingvik planet\\pycharmprojects\\newsapi\\natural_language_processing_app6\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kingvik planet\\pycharmprojects\\newsapi\\natural_language_processing_app6\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3.11 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31edbcc-d4cc-4512-8e65-57661b4c7dbf",
   "metadata": {},
   "source": [
    "#  Downloading NLTK stopwords corpus to my system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50739f7a-a608-4c56-bc2e-3370907d73f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\KingVik\n",
      "[nltk_data]     Planet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74efdf5e-c83e-4527-82fd-9b20683b51ef",
   "metadata": {},
   "source": [
    "# Importing stopwords from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd4d167-b63d-4e11-9512-3f6514ce1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccfe3f-5d97-402e-a347-b2653f43bdd7",
   "metadata": {},
   "source": [
    "# Listing English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc88334-f695-4e99-8cff-c5cadf2a48e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ee00b-f807-4e2a-9c5a-82db18835ad2",
   "metadata": {},
   "source": [
    "# Filtering out the English Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a32ff32-0cd8-4793-bfcb-46faee969423",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_list = []\n",
    "for count, word in d_list:\n",
    "    if word not in english_stopwords:\n",
    "        filtered_word_list.append((count, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002323e9-2fc5-485c-9590-6a4fd98c337f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(575, 'would'), (519, 'us'), (292, 'said'), (284, 'roberto'), (252, 'could')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_word_list = sorted(filtered_word_list, reverse = True)\n",
    "filtered_word_list[:5] # listing First 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fa1ac-eb30-4cbc-a345-cbd5938fa3d4",
   "metadata": {},
   "source": [
    "# Sentimental Analysis: what is the most Positive and Negative Chapter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "247d9ef0-c101-4eeb-88d7-84064ee2ae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\KingVik\n",
      "[nltk_data]     Planet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30002426-71ec-41d0-a9cd-75b5d0c17057",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8822b7a-e8f0-4d1c-bf67-361260e99aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_amplify_ep',\n",
       " '_amplify_qm',\n",
       " '_but_check',\n",
       " '_idioms_check',\n",
       " '_least_check',\n",
       " '_never_check',\n",
       " '_punctuation_emphasis',\n",
       " '_sift_sentiment_scores',\n",
       " 'constants',\n",
       " 'lexicon',\n",
       " 'lexicon_file',\n",
       " 'make_lex_dict',\n",
       " 'polarity_scores',\n",
       " 'score_valence',\n",
       " 'sentiment_valence']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045e258-9ed1-4936-a9be-82cabc089869",
   "metadata": {},
   "source": [
    "# Using Polarity_score find the Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "980a3e07-d9eb-4a16-b236-eac6e39c941e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.5994}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = analyzer.polarity_scores(\"what a beautiful nosense sunset\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf2cf9-12a3-462a-8044-f29e721df0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scores[\"pos\"] > scores[\"neg\"]:\n",
    "    print(\"Hey, It is a Postive Text\")\n",
    "else:\n",
    "    print(\"Hey, It is a Negative Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01aa110-eb6c-43a6-b700-7663e7c2eba5",
   "metadata": {},
   "source": [
    "# This take an Inputs and Analyse the Inputted words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "111afe66-5201-4250-b01e-c2e22d3e4801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input the word you want to analyze:  beautiful nosense\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, It is a Postive Text\n"
     ]
    }
   ],
   "source": [
    "# Taking user input for the word to analyze\n",
    "word = input(\"Input the word you want to analyze: \")\n",
    "\n",
    "# Analyzing the sentiment of the input word\n",
    "scores = analyzer.polarity_scores(word)\n",
    "if scores[\"pos\"] > scores[\"neg\"]:\n",
    "    print(\"Hey, It is a Postive Text\")\n",
    "else:\n",
    "    print(\"Hey, It is a Negative Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe72be44-6052-4ab6-85b4-8181ba8834a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.116, 'neu': 0.76, 'pos': 0.125, 'compound': 1.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores(book)\n",
    "# book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f8818-39aa-45bd-a727-41be7bc171eb",
   "metadata": {},
   "source": [
    "# Find the sentiment analysis of the chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc9edba8-eb38-42fe-a16e-eeeea916706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(\"Chapter [0-9]+\")\n",
    "chapters = re.split(pattern, book)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1d53413-bb6e-4dd3-bc85-db8564a57714",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = chapters[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d0dbd10-e43f-4fa8-ad21-b4e1af72ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'neg': 0.061, 'neu': 0.779, 'pos': 0.16, 'compound': 1.0}\n",
      "2 {'neg': 0.12, 'neu': 0.726, 'pos': 0.154, 'compound': 0.9991}\n",
      "3 {'neg': 0.145, 'neu': 0.751, 'pos': 0.105, 'compound': -0.9999}\n",
      "4 {'neg': 0.141, 'neu': 0.721, 'pos': 0.138, 'compound': -0.9963}\n",
      "5 {'neg': 0.118, 'neu': 0.742, 'pos': 0.141, 'compound': 0.9997}\n",
      "6 {'neg': 0.124, 'neu': 0.761, 'pos': 0.115, 'compound': -0.9979}\n",
      "7 {'neg': 0.136, 'neu': 0.761, 'pos': 0.103, 'compound': -0.9999}\n",
      "8 {'neg': 0.12, 'neu': 0.786, 'pos': 0.094, 'compound': -0.9998}\n",
      "9 {'neg': 0.097, 'neu': 0.824, 'pos': 0.079, 'compound': -0.9996}\n",
      "10 {'neg': 0.086, 'neu': 0.733, 'pos': 0.181, 'compound': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for nr, chapter in enumerate(chapters):\n",
    "    scores = analyzer.polarity_scores(chapter)\n",
    "    print(nr + 1, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3d12ab3b-107b-405f-b307-454d8ff14684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 : {'neg': 0.061, 'neu': 0.779, 'pos': 0.16, 'compound': 1.0}\n",
      "Chapter 2 : {'neg': 0.12, 'neu': 0.726, 'pos': 0.154, 'compound': 0.9991}\n",
      "Chapter 3 : {'neg': 0.145, 'neu': 0.751, 'pos': 0.105, 'compound': -0.9999}\n",
      "Chapter 4 : {'neg': 0.141, 'neu': 0.721, 'pos': 0.138, 'compound': -0.9963}\n",
      "Chapter 5 : {'neg': 0.118, 'neu': 0.742, 'pos': 0.141, 'compound': 0.9997}\n",
      "Chapter 6 : {'neg': 0.124, 'neu': 0.761, 'pos': 0.115, 'compound': -0.9979}\n",
      "Chapter 7 : {'neg': 0.136, 'neu': 0.761, 'pos': 0.103, 'compound': -0.9999}\n",
      "Chapter 8 : {'neg': 0.12, 'neu': 0.786, 'pos': 0.094, 'compound': -0.9998}\n",
      "Chapter 9 : {'neg': 0.097, 'neu': 0.824, 'pos': 0.079, 'compound': -0.9996}\n",
      "Chapter 10 : {'neg': 0.086, 'neu': 0.733, 'pos': 0.181, 'compound': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Listing the Chapters\n",
    "for nr, chapter in enumerate(chapters, start=1):  # Start counting from 1\n",
    "    scores = analyzer.polarity_scores(chapter)\n",
    "    print(\"Chapter\", nr, \":\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a25b5c5c-785f-4921-99ad-f3ac20d1c65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 : {'neg': 0.061, 'neu': 0.779, 'pos': 0.16, 'compound': 1.0}\n",
      "Chapter 2 : {'neg': 0.12, 'neu': 0.726, 'pos': 0.154, 'compound': 0.9991}\n",
      "Chapter 3 : {'neg': 0.145, 'neu': 0.751, 'pos': 0.105, 'compound': -0.9999}\n",
      "Chapter 4 : {'neg': 0.141, 'neu': 0.721, 'pos': 0.138, 'compound': -0.9963}\n",
      "Chapter 5 : {'neg': 0.118, 'neu': 0.742, 'pos': 0.141, 'compound': 0.9997}\n",
      "Chapter 6 : {'neg': 0.124, 'neu': 0.761, 'pos': 0.115, 'compound': -0.9979}\n",
      "Chapter 7 : {'neg': 0.136, 'neu': 0.761, 'pos': 0.103, 'compound': -0.9999}\n",
      "Chapter 8 : {'neg': 0.12, 'neu': 0.786, 'pos': 0.094, 'compound': -0.9998}\n",
      "Chapter 9 : {'neg': 0.097, 'neu': 0.824, 'pos': 0.079, 'compound': -0.9996}\n",
      "Chapter 10 : {'neg': 0.086, 'neu': 0.733, 'pos': 0.181, 'compound': 1.0}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Listing the Chapters\n",
    "    for nr, chapter in enumerate(chapters, start=1):  # Start counting from 1\n",
    "        scores = analyzer.polarity_scores(chapter)\n",
    "        print(\"Chapter\", nr, \":\", scores)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4932dc-8565-4bb5-b061-cdb9541321df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
