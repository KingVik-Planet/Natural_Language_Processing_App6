{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b61c09-ed67-433d-a3e2-97c16e129e51",
   "metadata": {},
   "source": [
    "# Load the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ad62a9-0d7b-457f-b421-5c316891a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"What_I_believe.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    book = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0404bdc0-c268-4e16-9ebe-9f64807f67da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fcdfb0-b908-4977-bcdc-f3ab637b1d06",
   "metadata": {},
   "source": [
    "# Accessing or Getting Book Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "571b5b06-c8d7-4be2-a741-06567416a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: What I believe\n",
      "Author: Bertrand Russell\n",
      "Release date: June 6, 2024 [eBook #73782]\n",
      "Language: English\n"
     ]
    }
   ],
   "source": [
    "def Book_Info(book, keywords):\n",
    "    lines = book.split('\\n')\n",
    "    for line in lines:\n",
    "        for keyword in keywords:\n",
    "            if  keyword in line:\n",
    "                print(line)\n",
    "keywords = [\"Title\", \"Author\", \"Release date\", \"Language\"]\n",
    "Book_Info(book , keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a7fc85-7416-4d08-a3da-89d0468835db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a keyword to search or type 'exit' to Stop:  Author\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Bertrand Russell\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a keyword to search or type 'exit' to Stop:  exit\n"
     ]
    }
   ],
   "source": [
    "def Book_Info(book, Keywords):\n",
    "    lines = book.split('\\n')\n",
    "    while True:\n",
    "        keyword = input(\"Enter a keyword to search or type 'exit' to Stop: \")\n",
    "        if keyword.lower() == 'exit':\n",
    "            break\n",
    "        found = False\n",
    "        for line in lines:\n",
    "            if  keyword in line:\n",
    "                print(line)\n",
    "                found = True\n",
    "        if not found:\n",
    "            print(f' \"{keyword}\" does not exit in the book, Please enter another keyword or check properly.')\n",
    "\n",
    "Book_Info(book, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde00d9-5377-4923-98db-953996a7408b",
   "metadata": {},
   "source": [
    "# Numbers of Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b449646-0663-43c3-aac6-6b6b05c1c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.count(\"CHAPTER\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2151d8-f4ef-4fe6-a65d-81ad3d3e151a",
   "metadata": {},
   "source": [
    "# With Regular Expression (regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f38e38-7ba1-4a8a-a404-2a9003002905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67efd381-694a-4a1e-8c15-81992e5086d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'project', 'gutenberg', 'ebook', 'of']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(\"[a-zA-Z]+\")\n",
    "findings = re.findall(pattern, book.lower())\n",
    "findings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e20f05f-f6cc-40e6-a93b-15c91cbba6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 894,\n",
       " 'project': 88,\n",
       " 'gutenberg': 97,\n",
       " 'ebook': 13,\n",
       " 'of': 686,\n",
       " 'what': 44,\n",
       " 'i': 88,\n",
       " 'believe': 17,\n",
       " 'this': 159,\n",
       " 'is': 376,\n",
       " 'for': 108,\n",
       " 'use': 18,\n",
       " 'anyone': 5,\n",
       " 'anywhere': 2,\n",
       " 'in': 337,\n",
       " 'united': 16,\n",
       " 'states': 21,\n",
       " 'and': 351,\n",
       " 'most': 24,\n",
       " 'other': 52,\n",
       " 'parts': 3,\n",
       " 'world': 34,\n",
       " 'at': 59,\n",
       " 'no': 51,\n",
       " 'cost': 3,\n",
       " 'with': 112,\n",
       " 'almost': 5,\n",
       " 'restrictions': 3,\n",
       " 'whatsoever': 2,\n",
       " 'you': 79,\n",
       " 'may': 42,\n",
       " 'copy': 12,\n",
       " 'it': 174,\n",
       " 'give': 8,\n",
       " 'away': 5,\n",
       " 'or': 149,\n",
       " 're': 2,\n",
       " 'under': 10,\n",
       " 'terms': 23,\n",
       " 'license': 18,\n",
       " 'included': 4,\n",
       " 'online': 5,\n",
       " 'www': 10,\n",
       " 'org': 9,\n",
       " 'if': 75,\n",
       " 'are': 148,\n",
       " 'not': 151,\n",
       " 'located': 7,\n",
       " 'will': 45,\n",
       " 'have': 76,\n",
       " 'to': 506,\n",
       " 'check': 4,\n",
       " 'laws': 17,\n",
       " 'country': 5,\n",
       " 'where': 21,\n",
       " 'before': 7,\n",
       " 'using': 8,\n",
       " 'title': 1,\n",
       " 'author': 1,\n",
       " 'bertrand': 5,\n",
       " 'russell': 6,\n",
       " 'release': 1,\n",
       " 'date': 3,\n",
       " 'june': 3,\n",
       " 'language': 1,\n",
       " 'english': 2,\n",
       " 'original': 2,\n",
       " 'publication': 1,\n",
       " 'new': 11,\n",
       " 'york': 2,\n",
       " 'e': 30,\n",
       " 'p': 5,\n",
       " 'dutton': 4,\n",
       " 'company': 4,\n",
       " 'credits': 1,\n",
       " 'produced': 5,\n",
       " 'by': 134,\n",
       " 'donald': 1,\n",
       " 'cummings': 1,\n",
       " 'distributed': 5,\n",
       " 'proofreading': 1,\n",
       " 'team': 1,\n",
       " 'https': 1,\n",
       " 'pgdp': 1,\n",
       " 'net': 1,\n",
       " 'file': 2,\n",
       " 'was': 35,\n",
       " 'from': 55,\n",
       " 'images': 1,\n",
       " 'generously': 1,\n",
       " 'made': 5,\n",
       " 'available': 4,\n",
       " 'internet': 1,\n",
       " 'archive': 14,\n",
       " 'american': 1,\n",
       " 'libraries': 1,\n",
       " 'start': 3,\n",
       " 'day': 2,\n",
       " 'morrow': 1,\n",
       " 'series': 2,\n",
       " 'd': 8,\n",
       " 'dalus': 1,\n",
       " 'science': 29,\n",
       " 'future': 16,\n",
       " 'j': 3,\n",
       " 'b': 7,\n",
       " 's': 62,\n",
       " 'haldane': 2,\n",
       " 'icarus': 3,\n",
       " 'hon': 3,\n",
       " 'f': 20,\n",
       " 'r': 5,\n",
       " 'mongol': 1,\n",
       " 'our': 46,\n",
       " 'midst': 1,\n",
       " 'g': 2,\n",
       " 'crookshank': 2,\n",
       " 'm': 8,\n",
       " 'fully': 3,\n",
       " 'illustrated': 2,\n",
       " 'wireless': 2,\n",
       " 'possibilities': 3,\n",
       " 'prof': 1,\n",
       " 'a': 321,\n",
       " 'low': 1,\n",
       " 'four': 1,\n",
       " 'diagrams': 1,\n",
       " 'tantalus': 1,\n",
       " 'man': 31,\n",
       " 'c': 7,\n",
       " 'schiller': 1,\n",
       " 'narcissus': 1,\n",
       " 'an': 57,\n",
       " 'anatomy': 1,\n",
       " 'clothes': 3,\n",
       " 'gerald': 1,\n",
       " 'heard': 2,\n",
       " 'callinicus': 1,\n",
       " 'defence': 1,\n",
       " 'chemical': 2,\n",
       " 'warfare': 1,\n",
       " 'passing': 1,\n",
       " 'phantoms': 1,\n",
       " 'patten': 1,\n",
       " 'sc': 2,\n",
       " 'quo': 1,\n",
       " 'vadimus': 1,\n",
       " 'some': 24,\n",
       " 'glimpses': 1,\n",
       " 'fournier': 1,\n",
       " 'albe': 1,\n",
       " 'inst': 1,\n",
       " 'conquest': 1,\n",
       " 'cancer': 2,\n",
       " 'h': 2,\n",
       " 'w': 1,\n",
       " 'wright': 1,\n",
       " 'lysistrata': 1,\n",
       " 'woman': 5,\n",
       " 'anthony': 1,\n",
       " 'ludovici': 1,\n",
       " 'hypatia': 1,\n",
       " 'dora': 1,\n",
       " 'mrs': 1,\n",
       " 'preparation': 1,\n",
       " 'perseus': 1,\n",
       " 'dragons': 1,\n",
       " 'scott': 1,\n",
       " 'stokes': 1,\n",
       " 'evocation': 1,\n",
       " 'genius': 1,\n",
       " 'alan': 1,\n",
       " 'porter': 1,\n",
       " 'sex': 7,\n",
       " 'rebecca': 1,\n",
       " 'west': 4,\n",
       " 'aesculapius': 1,\n",
       " 'disease': 7,\n",
       " 'illustration': 2,\n",
       " 'fifth': 1,\n",
       " 'avenue': 1,\n",
       " 'copyright': 21,\n",
       " 'all': 64,\n",
       " 'rights': 3,\n",
       " 'reserved': 1,\n",
       " 'first': 5,\n",
       " 'printing': 4,\n",
       " 'april': 1,\n",
       " 'second': 4,\n",
       " 'third': 1,\n",
       " 'fourth': 1,\n",
       " 'march': 1,\n",
       " 'printed': 3,\n",
       " 'america': 1,\n",
       " 'preface': 1,\n",
       " 'little': 8,\n",
       " 'book': 2,\n",
       " 'tried': 1,\n",
       " 'say': 10,\n",
       " 'think': 22,\n",
       " 'place': 3,\n",
       " 'universe': 2,\n",
       " 'his': 17,\n",
       " 'way': 23,\n",
       " 'achieving': 2,\n",
       " 'good': 56,\n",
       " 'life': 67,\n",
       " 'expressed': 3,\n",
       " 'my': 12,\n",
       " 'fears': 4,\n",
       " 'following': 5,\n",
       " 'pages': 2,\n",
       " 'hopes': 4,\n",
       " 'inconsistency': 1,\n",
       " 'only': 29,\n",
       " 'apparent': 1,\n",
       " 'except': 14,\n",
       " 'astronomy': 3,\n",
       " 'mankind': 6,\n",
       " 'achieved': 6,\n",
       " 'art': 4,\n",
       " 'predicting': 1,\n",
       " 'human': 34,\n",
       " 'affairs': 1,\n",
       " 'we': 120,\n",
       " 'can': 51,\n",
       " 'see': 11,\n",
       " 'that': 197,\n",
       " 'there': 57,\n",
       " 'forces': 6,\n",
       " 'making': 5,\n",
       " 'happiness': 13,\n",
       " 'misery': 3,\n",
       " 'do': 61,\n",
       " 'know': 9,\n",
       " 'which': 103,\n",
       " 'prevail': 1,\n",
       " 'but': 95,\n",
       " 'act': 4,\n",
       " 'wisely': 2,\n",
       " 'must': 41,\n",
       " 'be': 188,\n",
       " 'aware': 1,\n",
       " 'both': 14,\n",
       " 'january': 1,\n",
       " 'st': 1,\n",
       " 'table': 2,\n",
       " 'contents': 1,\n",
       " 'chap': 1,\n",
       " 'page': 3,\n",
       " 'nature': 50,\n",
       " 'ii': 1,\n",
       " 'iii': 1,\n",
       " 'moral': 15,\n",
       " 'rules': 11,\n",
       " 'iv': 1,\n",
       " 'salvation': 11,\n",
       " 'individual': 17,\n",
       " 'social': 18,\n",
       " 'v': 1,\n",
       " 'chapter': 10,\n",
       " 'part': 29,\n",
       " 'something': 9,\n",
       " 'contrasted': 1,\n",
       " 'thoughts': 2,\n",
       " 'bodily': 6,\n",
       " 'movements': 1,\n",
       " 'follow': 5,\n",
       " 'same': 14,\n",
       " 'describe': 4,\n",
       " 'motions': 2,\n",
       " 'stars': 2,\n",
       " 'atoms': 1,\n",
       " 'physical': 16,\n",
       " 'large': 8,\n",
       " 'compared': 1,\n",
       " 'larger': 2,\n",
       " 'than': 50,\n",
       " 'thought': 10,\n",
       " 'dante': 1,\n",
       " 'time': 8,\n",
       " 'so': 37,\n",
       " 'as': 126,\n",
       " 'seemed': 1,\n",
       " 'hundred': 2,\n",
       " 'years': 5,\n",
       " 'ago': 1,\n",
       " 'upward': 1,\n",
       " 'downward': 1,\n",
       " 'small': 7,\n",
       " 'seems': 6,\n",
       " 'reaching': 2,\n",
       " 'limits': 3,\n",
       " 'finite': 4,\n",
       " 'extent': 8,\n",
       " 'space': 2,\n",
       " 'light': 3,\n",
       " 'could': 17,\n",
       " 'travel': 3,\n",
       " 'round': 1,\n",
       " 'few': 3,\n",
       " 'millions': 2,\n",
       " 'matter': 20,\n",
       " 'consists': 6,\n",
       " 'electrons': 4,\n",
       " 'protons': 4,\n",
       " 'size': 1,\n",
       " 'number': 8,\n",
       " 'probably': 6,\n",
       " 'their': 41,\n",
       " 'changes': 3,\n",
       " 'continuous': 1,\n",
       " 'used': 10,\n",
       " 'proceed': 1,\n",
       " 'jerks': 1,\n",
       " 'never': 6,\n",
       " 'smaller': 4,\n",
       " 'certain': 26,\n",
       " 'minimum': 2,\n",
       " 'jerk': 1,\n",
       " 'these': 34,\n",
       " 'apparently': 1,\n",
       " 'summed': 1,\n",
       " 'up': 9,\n",
       " 'very': 24,\n",
       " 'general': 8,\n",
       " 'principles': 1,\n",
       " 'determine': 5,\n",
       " 'past': 3,\n",
       " 'when': 39,\n",
       " 'any': 59,\n",
       " 'section': 9,\n",
       " 'its': 28,\n",
       " 'history': 7,\n",
       " 'known': 3,\n",
       " 'thus': 8,\n",
       " 'approaching': 1,\n",
       " 'stage': 3,\n",
       " 'complete': 5,\n",
       " 'therefore': 14,\n",
       " 'uninteresting': 4,\n",
       " 'given': 6,\n",
       " 'governing': 1,\n",
       " 'rest': 2,\n",
       " 'merely': 14,\n",
       " 'geography': 2,\n",
       " 'collection': 5,\n",
       " 'particular': 6,\n",
       " 'facts': 11,\n",
       " 'telling': 1,\n",
       " 'distribution': 7,\n",
       " 'throughout': 4,\n",
       " 'portion': 3,\n",
       " 'total': 3,\n",
       " 'required': 4,\n",
       " 'theoretically': 1,\n",
       " 'they': 82,\n",
       " 'written': 4,\n",
       " 'down': 3,\n",
       " 'big': 1,\n",
       " 'kept': 1,\n",
       " 'somerset': 1,\n",
       " 'house': 1,\n",
       " 'calculating': 1,\n",
       " 'machine': 2,\n",
       " 'attached': 5,\n",
       " 'turning': 1,\n",
       " 'handle': 1,\n",
       " 'would': 31,\n",
       " 'enable': 2,\n",
       " 'inquirer': 1,\n",
       " 'find': 10,\n",
       " 'out': 10,\n",
       " 'times': 3,\n",
       " 'those': 20,\n",
       " 'recorded': 1,\n",
       " 'difficult': 6,\n",
       " 'imagine': 2,\n",
       " 'anything': 9,\n",
       " 'less': 9,\n",
       " 'interesting': 4,\n",
       " 'more': 50,\n",
       " 'different': 12,\n",
       " 'passionate': 1,\n",
       " 'delights': 1,\n",
       " 'incomplete': 2,\n",
       " 'discovery': 3,\n",
       " 'like': 14,\n",
       " 'climbing': 1,\n",
       " 'high': 1,\n",
       " 'mountain': 1,\n",
       " 'finding': 2,\n",
       " 'nothing': 13,\n",
       " 'top': 1,\n",
       " 'restaurant': 1,\n",
       " 'sell': 1,\n",
       " 'ginger': 1,\n",
       " 'beer': 1,\n",
       " 'surrounded': 1,\n",
       " 'fog': 1,\n",
       " 'equipped': 1,\n",
       " 'perhaps': 6,\n",
       " 'ahmes': 1,\n",
       " 'multiplication': 1,\n",
       " 'exciting': 2,\n",
       " 'itself': 3,\n",
       " 'body': 4,\n",
       " 'composed': 2,\n",
       " 'far': 17,\n",
       " 'obey': 2,\n",
       " 'forming': 1,\n",
       " 'animals': 1,\n",
       " 'plants': 1,\n",
       " 'who': 36,\n",
       " 'maintain': 2,\n",
       " 'physiology': 2,\n",
       " 'reduced': 2,\n",
       " 'physics': 1,\n",
       " 'arguments': 4,\n",
       " 'convincing': 1,\n",
       " 'prudent': 1,\n",
       " 'suppose': 6,\n",
       " 'mistaken': 1,\n",
       " 'call': 4,\n",
       " 'seem': 7,\n",
       " 'depend': 5,\n",
       " 'upon': 23,\n",
       " 'organization': 3,\n",
       " 'tracks': 2,\n",
       " 'brain': 5,\n",
       " 'sort': 8,\n",
       " 'journeys': 1,\n",
       " 'roads': 2,\n",
       " 'railways': 2,\n",
       " 'energy': 4,\n",
       " 'thinking': 4,\n",
       " 'origin': 3,\n",
       " 'instance': 8,\n",
       " 'deficiency': 1,\n",
       " 'iodine': 1,\n",
       " 'turn': 1,\n",
       " 'clever': 1,\n",
       " 'into': 4,\n",
       " 'idiot': 1,\n",
       " 'mental': 3,\n",
       " 'phenomena': 3,\n",
       " 'bound': 4,\n",
       " 'material': 1,\n",
       " 'structure': 2,\n",
       " 'cannot': 30,\n",
       " 'solitary': 2,\n",
       " 'electron': 1,\n",
       " 'proton': 1,\n",
       " 'might': 7,\n",
       " 'well': 8,\n",
       " 'expect': 1,\n",
       " 'play': 4,\n",
       " 'football': 1,\n",
       " 'match': 1,\n",
       " 'also': 12,\n",
       " 'survives': 1,\n",
       " 'death': 13,\n",
       " 'since': 17,\n",
       " 'destroys': 1,\n",
       " 'dissipates': 1,\n",
       " 'utilized': 1,\n",
       " 'god': 13,\n",
       " 'immortality': 8,\n",
       " 'central': 1,\n",
       " 'dogmas': 2,\n",
       " 'christian': 7,\n",
       " 'religion': 7,\n",
       " 'support': 7,\n",
       " 'said': 10,\n",
       " 'either': 12,\n",
       " 'doctrine': 3,\n",
       " 'essential': 3,\n",
       " 'neither': 7,\n",
       " 'found': 4,\n",
       " 'buddhism': 1,\n",
       " 'regard': 7,\n",
       " 'statement': 2,\n",
       " 'unqualified': 1,\n",
       " 'form': 12,\n",
       " 'misleading': 1,\n",
       " 'correct': 2,\n",
       " 'last': 4,\n",
       " 'analysis': 2,\n",
       " 'come': 7,\n",
       " 'them': 33,\n",
       " 'irreducible': 1,\n",
       " 'theology': 1,\n",
       " 'doubt': 5,\n",
       " 'people': 29,\n",
       " 'continue': 2,\n",
       " 'entertain': 1,\n",
       " 'beliefs': 2,\n",
       " 'because': 20,\n",
       " 'pleasant': 4,\n",
       " 'just': 3,\n",
       " 'ourselves': 4,\n",
       " 'virtuous': 3,\n",
       " 'enemies': 3,\n",
       " 'wicked': 7,\n",
       " 'ground': 3,\n",
       " 'pretend': 1,\n",
       " 'able': 6,\n",
       " 'prove': 5,\n",
       " 'equally': 6,\n",
       " 'satan': 1,\n",
       " 'fiction': 1,\n",
       " 'exist': 5,\n",
       " 'gods': 2,\n",
       " 'olympus': 1,\n",
       " 'ancient': 4,\n",
       " 'egypt': 1,\n",
       " 'babylon': 1,\n",
       " 'one': 45,\n",
       " 'hypotheses': 1,\n",
       " 'probable': 4,\n",
       " 'lie': 1,\n",
       " 'outside': 5,\n",
       " 'region': 1,\n",
       " 'even': 24,\n",
       " 'knowledge': 36,\n",
       " 'reason': 5,\n",
       " 'consider': 4,\n",
       " 'shall': 18,\n",
       " 'enlarge': 1,\n",
       " 'question': 11,\n",
       " 'dealt': 2,\n",
       " 'elsewhere': 4,\n",
       " 'philosophy': 10,\n",
       " 'leibniz': 1,\n",
       " 'xv': 1,\n",
       " 'personal': 2,\n",
       " 'stands': 1,\n",
       " 'on': 50,\n",
       " 'somewhat': 2,\n",
       " 'footing': 1,\n",
       " 'here': 8,\n",
       " 'evidence': 8,\n",
       " 'possible': 18,\n",
       " 'persons': 2,\n",
       " 'everyday': 1,\n",
       " 'concerned': 6,\n",
       " 'conditions': 3,\n",
       " 'existence': 1,\n",
       " 'discoverable': 1,\n",
       " 'drop': 2,\n",
       " 'water': 2,\n",
       " 'immortal': 3,\n",
       " 'resolved': 1,\n",
       " 'oxygen': 1,\n",
       " 'hydrogen': 1,\n",
       " 'were': 25,\n",
       " 'had': 20,\n",
       " 'quality': 1,\n",
       " 'aqueousness': 1,\n",
       " 'survive': 5,\n",
       " 'dissolution': 1,\n",
       " 'should': 38,\n",
       " 'inclined': 1,\n",
       " 'sceptical': 1,\n",
       " 'manner': 2,\n",
       " 'organized': 3,\n",
       " 'living': 3,\n",
       " 'becomes': 4,\n",
       " 'demobilized': 1,\n",
       " 'collective': 3,\n",
       " 'action': 1,\n",
       " 'goes': 1,\n",
       " 'show': 5,\n",
       " 'rational': 5,\n",
       " 'ceases': 2,\n",
       " 'argument': 5,\n",
       " 'probability': 2,\n",
       " 'strong': 2,\n",
       " 'scientific': 12,\n",
       " 'conclusions': 1,\n",
       " 'based': 5,\n",
       " 'various': 1,\n",
       " 'grounds': 4,\n",
       " 'conclusion': 1,\n",
       " 'attacked': 1,\n",
       " 'psychical': 4,\n",
       " 'research': 6,\n",
       " 'professes': 1,\n",
       " 'actual': 4,\n",
       " 'survival': 5,\n",
       " 'undoubtedly': 3,\n",
       " 'procedure': 1,\n",
       " 'principle': 5,\n",
       " 'scientifically': 1,\n",
       " 'overwhelming': 1,\n",
       " 'temper': 1,\n",
       " 'reject': 1,\n",
       " 'weight': 1,\n",
       " 'however': 13,\n",
       " 'antecedent': 1,\n",
       " 'hypothesis': 1,\n",
       " 'always': 6,\n",
       " 'ways': 11,\n",
       " 'accounting': 1,\n",
       " 'set': 11,\n",
       " 'prefer': 4,\n",
       " 'antecedently': 1,\n",
       " 'least': 5,\n",
       " 'improbable': 1,\n",
       " 'already': 7,\n",
       " 'likely': 11,\n",
       " 'ready': 1,\n",
       " 'view': 14,\n",
       " 'theory': 3,\n",
       " 'best': 13,\n",
       " 'explanation': 3,\n",
       " 'unplausible': 1,\n",
       " 'seek': 2,\n",
       " 'explanations': 1,\n",
       " 'adduced': 1,\n",
       " 'favour': 3,\n",
       " 'much': 16,\n",
       " 'weaker': 1,\n",
       " 'physiological': 6,\n",
       " 'side': 6,\n",
       " 'admit': 2,\n",
       " 'moment': 5,\n",
       " 'become': 8,\n",
       " 'stronger': 3,\n",
       " 'case': 9,\n",
       " 'unscientific': 3,\n",
       " 'disbelieve': 1,\n",
       " 'mean': 9,\n",
       " 'postponement': 1,\n",
       " 'men': 32,\n",
       " 'desire': 30,\n",
       " 'believers': 1,\n",
       " 'object': 7,\n",
       " 'such': 31,\n",
       " 'been': 17,\n",
       " 'soul': 9,\n",
       " 'totally': 1,\n",
       " 'disparate': 1,\n",
       " 'quite': 11,\n",
       " 'empirical': 1,\n",
       " 'manifestations': 1,\n",
       " 'through': 8,\n",
       " 'organs': 1,\n",
       " 'metaphysical': 1,\n",
       " 'superstition': 12,\n",
       " 'mind': 5,\n",
       " 'alike': 1,\n",
       " 'purposes': 2,\n",
       " 'convenient': 1,\n",
       " 'ultimate': 4,\n",
       " 'realities': 1,\n",
       " 'logical': 3,\n",
       " 'fictions': 1,\n",
       " 'each': 14,\n",
       " 'really': 3,\n",
       " 'events': 3,\n",
       " 'single': 2,\n",
       " 'persistent': 1,\n",
       " 'entity': 4,\n",
       " 'obvious': 5,\n",
       " 'growth': 1,\n",
       " 'whoever': 1,\n",
       " 'considers': 1,\n",
       " 'conception': 15,\n",
       " 'gestation': 1,\n",
       " 'infancy': 3,\n",
       " 'seriously': 1,\n",
       " 'indivisible': 2,\n",
       " 'perfect': 3,\n",
       " 'process': 4,\n",
       " 'evident': 2,\n",
       " 'grows': 1,\n",
       " 'derives': 1,\n",
       " 'spermatozoon': 1,\n",
       " 'ovum': 1,\n",
       " 'materialism': 1,\n",
       " 'recognition': 2,\n",
       " 'everything': 3,\n",
       " 'primal': 1,\n",
       " 'substance': 1,\n",
       " 'metaphysicians': 2,\n",
       " 'advanced': 1,\n",
       " 'innumerable': 1,\n",
       " 'simple': 2,\n",
       " 'test': 1,\n",
       " 'demolished': 1,\n",
       " 'pervade': 1,\n",
       " 'anxious': 1,\n",
       " 'fat': 1,\n",
       " 'live': 11,\n",
       " 'long': 6,\n",
       " 'none': 3,\n",
       " 'ever': 3,\n",
       " 'noticed': 1,\n",
       " 'application': 4,\n",
       " 'reasonings': 1,\n",
       " 'amazing': 1,\n",
       " 'power': 7,\n",
       " 'blinding': 1,\n",
       " 'fallacies': 1,\n",
       " 'otherwise': 4,\n",
       " 'once': 1,\n",
       " 'afraid': 1,\n",
       " 'idea': 5,\n",
       " 'arisen': 1,\n",
       " 'fear': 22,\n",
       " 'basis': 4,\n",
       " 'religious': 3,\n",
       " 'dogma': 1,\n",
       " 'else': 4,\n",
       " 'beings': 9,\n",
       " 'individually': 1,\n",
       " 'collectively': 1,\n",
       " 'dominates': 1,\n",
       " 'gives': 3,\n",
       " 'rise': 2,\n",
       " 'antithesis': 3,\n",
       " 'seen': 2,\n",
       " 'illusory': 1,\n",
       " 'another': 19,\n",
       " 'important': 6,\n",
       " 'namely': 4,\n",
       " 'between': 9,\n",
       " 'things': 17,\n",
       " 'affected': 2,\n",
       " 'desires': 33,\n",
       " 'line': 1,\n",
       " 'two': 14,\n",
       " 'sharp': 1,\n",
       " 'nor': 10,\n",
       " 'immutable': 1,\n",
       " 'advances': 1,\n",
       " 'brought': 3,\n",
       " 'control': 9,\n",
       " 'nevertheless': 5,\n",
       " 'remain': 3,\n",
       " 'definitely': 1,\n",
       " 'among': 7,\n",
       " 'near': 1,\n",
       " 'surface': 3,\n",
       " 'earth': 5,\n",
       " 'mould': 2,\n",
       " 'suit': 1,\n",
       " 'powers': 2,\n",
       " 'limited': 6,\n",
       " 'above': 2,\n",
       " 'prevent': 5,\n",
       " 'although': 3,\n",
       " 'often': 5,\n",
       " 'delay': 1,\n",
       " 'attempt': 3,\n",
       " 'overcome': 2,\n",
       " 'controlled': 1,\n",
       " 'moved': 1,\n",
       " 'prayer': 2,\n",
       " 'acquire': 1,\n",
       " 'share': 3,\n",
       " 'omnipotence': 1,\n",
       " 'former': 4,\n",
       " 'days': 5,\n",
       " 'miracles': 2,\n",
       " 'happened': 1,\n",
       " 'answer': 3,\n",
       " 'still': 5,\n",
       " 'catholic': 1,\n",
       " 'church': 5,\n",
       " 'protestants': 2,\n",
       " 'lost': 1,\n",
       " 'dispense': 1,\n",
       " 'providence': 1,\n",
       " 'has': 34,\n",
       " 'decreed': 2,\n",
       " 'operation': 2,\n",
       " 'natural': 7,\n",
       " 'produce': 5,\n",
       " 'results': 4,\n",
       " 'belief': 6,\n",
       " 'serves': 1,\n",
       " 'humanize': 1,\n",
       " 'make': 18,\n",
       " 'feel': 7,\n",
       " 'allies': 1,\n",
       " 'removes': 1,\n",
       " 'terror': 4,\n",
       " 'die': 8,\n",
       " 'inherit': 2,\n",
       " 'eternal': 1,\n",
       " 'bliss': 1,\n",
       " 'expected': 2,\n",
       " 'without': 20,\n",
       " 'horror': 1,\n",
       " 'though': 4,\n",
       " 'fortunately': 1,\n",
       " 'medical': 2,\n",
       " 'does': 18,\n",
       " 'invariably': 1,\n",
       " 'happen': 1,\n",
       " 'soothe': 1,\n",
       " 'allay': 1,\n",
       " 'wholly': 5,\n",
       " 'source': 5,\n",
       " 'dignified': 1,\n",
       " 'kinds': 3,\n",
       " 'disgraceful': 1,\n",
       " 'done': 10,\n",
       " 'great': 17,\n",
       " 'disservice': 1,\n",
       " 'bad': 13,\n",
       " 'ought': 13,\n",
       " 'fairy': 1,\n",
       " 'tales': 1,\n",
       " 'courage': 23,\n",
       " 'reflection': 1,\n",
       " 'rot': 1,\n",
       " 'ego': 1,\n",
       " 'am': 8,\n",
       " 'young': 6,\n",
       " 'love': 38,\n",
       " 'scorn': 1,\n",
       " 'shiver': 2,\n",
       " 'annihilation': 1,\n",
       " 'true': 5,\n",
       " 'end': 9,\n",
       " 'lose': 2,\n",
       " 'value': 11,\n",
       " 'everlasting': 1,\n",
       " 'many': 11,\n",
       " 'borne': 2,\n",
       " 'himself': 3,\n",
       " 'proudly': 1,\n",
       " 'scaffold': 1,\n",
       " 'surely': 4,\n",
       " 'pride': 1,\n",
       " 'teach': 1,\n",
       " 'us': 16,\n",
       " 'truly': 1,\n",
       " 'about': 14,\n",
       " 'open': 2,\n",
       " 'windows': 1,\n",
       " 'after': 4,\n",
       " 'cosy': 1,\n",
       " 'indoor': 1,\n",
       " 'warmth': 1,\n",
       " 'traditional': 4,\n",
       " 'humanizing': 1,\n",
       " 'myths': 1,\n",
       " 'fresh': 1,\n",
       " 'air': 1,\n",
       " 'brings': 2,\n",
       " 'vigour': 1,\n",
       " 'spaces': 1,\n",
       " 'splendour': 1,\n",
       " 'own': 10,\n",
       " 'thing': 4,\n",
       " 'harm': 14,\n",
       " 'confusing': 1,\n",
       " 'bearing': 1,\n",
       " 'whatever': 2,\n",
       " 'hand': 1,\n",
       " 'forbidden': 2,\n",
       " 'non': 2,\n",
       " 'compelled': 1,\n",
       " 'admire': 2,\n",
       " 'law': 12,\n",
       " 'accordance': 3,\n",
       " 'physicist': 1,\n",
       " 'beginning': 2,\n",
       " 'discover': 3,\n",
       " 'sense': 8,\n",
       " 'subordinated': 1,\n",
       " 'outcome': 1,\n",
       " 'victims': 4,\n",
       " 'run': 2,\n",
       " 'unduly': 1,\n",
       " 'terrestrial': 1,\n",
       " 'planets': 1,\n",
       " 'milky': 1,\n",
       " 'ridiculous': 1,\n",
       " 'warp': 1,\n",
       " 'order': 6,\n",
       " 'bring': 4,\n",
       " 'pleasing': 1,\n",
       " 'tiny': 1,\n",
       " 'parasites': 1,\n",
       " 'insignificant': 1,\n",
       " 'planet': 1,\n",
       " 'vitalism': 1,\n",
       " 'evolutionism': 1,\n",
       " 'respect': 6,\n",
       " 'lack': 4,\n",
       " 'proportion': 4,\n",
       " 'relevance': 1,\n",
       " 'personally': 1,\n",
       " 'having': 3,\n",
       " 'cosmic': 2,\n",
       " 'significance': 2,\n",
       " 'confined': 2,\n",
       " 'optimism': 1,\n",
       " 'pessimism': 1,\n",
       " 'philosophies': 2,\n",
       " 'na': 1,\n",
       " 've': 1,\n",
       " 'humanism': 1,\n",
       " 'happy': 2,\n",
       " 'unhappy': 1,\n",
       " 'spring': 1,\n",
       " 'self': 9,\n",
       " 'importance': 5,\n",
       " 'corrected': 2,\n",
       " 'situation': 2,\n",
       " 'reversed': 1,\n",
       " 'real': 3,\n",
       " 'imagined': 2,\n",
       " 'appraised': 1,\n",
       " 'standard': 2,\n",
       " 'valuation': 1,\n",
       " 'wrong': 2,\n",
       " 'irrefutable': 1,\n",
       " 'arbiters': 1,\n",
       " 'greater': 3,\n",
       " 'values': 1,\n",
       " 'neutral': 1,\n",
       " 'deserving': 1,\n",
       " 'admiration': 3,\n",
       " 'censure': 2,\n",
       " 'create': 1,\n",
       " 'confer': 2,\n",
       " 'realm': 2,\n",
       " 'kings': 1,\n",
       " 'debase': 1,\n",
       " 'kingship': 1,\n",
       " 'bow': 1,\n",
       " 'personified': 1,\n",
       " 'varying': 2,\n",
       " 'conceptions': 2,\n",
       " 'differences': 2,\n",
       " 'amenable': 1,\n",
       " 'differed': 1,\n",
       " 'means': 15,\n",
       " 'achieve': 3,\n",
       " 'prison': 2,\n",
       " 'preventing': 7,\n",
       " 'crime': 4,\n",
       " 'others': 11,\n",
       " 'hold': 4,\n",
       " 'education': 15,\n",
       " 'better': 17,\n",
       " 'difference': 3,\n",
       " 'decided': 2,\n",
       " 'sufficient': 4,\n",
       " 'tested': 2,\n",
       " 'tolstoy': 2,\n",
       " 'condemned': 2,\n",
       " 'war': 10,\n",
       " 'held': 1,\n",
       " 'soldier': 3,\n",
       " 'doing': 1,\n",
       " 'battle': 2,\n",
       " 'right': 12,\n",
       " 'noble': 1,\n",
       " 'involved': 4,\n",
       " 'ends': 9,\n",
       " 'praise': 3,\n",
       " 'usually': 4,\n",
       " 'punishment': 6,\n",
       " 'sinners': 1,\n",
       " 'did': 2,\n",
       " 'state': 11,\n",
       " 'hope': 4,\n",
       " 'agree': 12,\n",
       " 'inspired': 3,\n",
       " 'guided': 4,\n",
       " 'indefinitely': 1,\n",
       " 'extensible': 1,\n",
       " 'middle': 3,\n",
       " 'ages': 1,\n",
       " 'pestilence': 2,\n",
       " 'appeared': 1,\n",
       " 'holy': 1,\n",
       " 'advised': 1,\n",
       " 'population': 4,\n",
       " 'assemble': 1,\n",
       " 'churches': 1,\n",
       " 'pray': 1,\n",
       " 'deliverance': 1,\n",
       " 'result': 3,\n",
       " 'infection': 1,\n",
       " 'spread': 3,\n",
       " 'extraordinary': 1,\n",
       " 'rapidity': 1,\n",
       " 'crowded': 1,\n",
       " 'masses': 1,\n",
       " 'supplicants': 1,\n",
       " 'example': 8,\n",
       " 'late': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d  = {}\n",
    "for word in findings:\n",
    "    if word in d.keys():\n",
    "        d[word] = d[word] + 1\n",
    "    else:\n",
    "        d[word] = 1\n",
    "        \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7950356-efd6-464b-831a-fe63e7bb2a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 894), ('project', 88), ('of', 686), ('gutenberg', 97), ('ebook', 13)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_list = [(value, key) for (value, key) in d.items()]\n",
    "sorted(d_list[:5], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1fb15-8a9e-4577-a2a5-5c3c60d76e4f",
   "metadata": {},
   "source": [
    "# Checking python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9aa39d0-6e15-43f0-8148-7ae6abbf1f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.12.4'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02959d4-912c-4fde-9219-631e8e2ddfcd",
   "metadata": {},
   "source": [
    "# Installing nltk libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561f8298-8cdc-4042-8ded-9b39ca0f65ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\chukw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\chukw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\chukw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\chukw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\chukw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\chukw\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3.12 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31edbcc-d4cc-4512-8e65-57661b4c7dbf",
   "metadata": {},
   "source": [
    "#  Downloading NLTK stopwords corpus to my system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50739f7a-a608-4c56-bc2e-3370907d73f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chukw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74efdf5e-c83e-4527-82fd-9b20683b51ef",
   "metadata": {},
   "source": [
    "# Importing stopwords from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dd4d167-b63d-4e11-9512-3f6514ce1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccfe3f-5d97-402e-a347-b2653f43bdd7",
   "metadata": {},
   "source": [
    "# Listing English Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dc88334-f695-4e99-8cff-c5cadf2a48e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ee00b-f807-4e2a-9c5a-82db18835ad2",
   "metadata": {},
   "source": [
    "# Filtering out the English Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a32ff32-0cd8-4793-bfcb-46faee969423",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_list = []\n",
    "for count, word in d_list:\n",
    "    if word not in english_stopwords:\n",
    "        filtered_word_list.append((count, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "002323e9-2fc5-485c-9590-6a4fd98c337f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('youth', 2), ('your', 15), ('young', 6), ('you', 79), ('york', 2)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_word_list = sorted(filtered_word_list, reverse = True)\n",
    "filtered_word_list[:5] # listing First 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fa1ac-eb30-4cbc-a345-cbd5938fa3d4",
   "metadata": {},
   "source": [
    "# Sentimental Analysis: what is the most Positive and Negative Chapter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "247d9ef0-c101-4eeb-88d7-84064ee2ae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\chukw\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30002426-71ec-41d0-a9cd-75b5d0c17057",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8822b7a-e8f0-4d1c-bf67-361260e99aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_amplify_ep',\n",
       " '_amplify_qm',\n",
       " '_but_check',\n",
       " '_idioms_check',\n",
       " '_least_check',\n",
       " '_never_check',\n",
       " '_punctuation_emphasis',\n",
       " '_sift_sentiment_scores',\n",
       " 'constants',\n",
       " 'lexicon',\n",
       " 'lexicon_file',\n",
       " 'make_lex_dict',\n",
       " 'polarity_scores',\n",
       " 'score_valence',\n",
       " 'sentiment_valence']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045e258-9ed1-4936-a9be-82cabc089869",
   "metadata": {},
   "source": [
    "# Using Polarity_score find the Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "980a3e07-d9eb-4a16-b236-eac6e39c941e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.5994}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = analyzer.polarity_scores(\"what a beautiful nosense sunset\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eaf2cf9-12a3-462a-8044-f29e721df0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, It is a Postive Text\n"
     ]
    }
   ],
   "source": [
    "if scores[\"pos\"] > scores[\"neg\"]:\n",
    "    print(\"Hey, It is a Postive Text\")\n",
    "else:\n",
    "    print(\"Hey, It is a Negative Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01aa110-eb6c-43a6-b700-7663e7c2eba5",
   "metadata": {},
   "source": [
    "# This take an Inputs and Analyse the Inputted words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "111afe66-5201-4250-b01e-c2e22d3e4801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input the word you want to analyze:  I hate you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.787, 'neu': 0.213, 'pos': 0.0, 'compound': -0.5719}\n",
      "Hey, It is a Negative Text\n"
     ]
    }
   ],
   "source": [
    "# Taking user input for the word to analyze\n",
    "word = input(\"Input the word you want to analyze: \")\n",
    "\n",
    "# Analyzing the sentiment of the input word\n",
    "scores = analyzer.polarity_scores(word)\n",
    "print(scores)\n",
    "if scores[\"pos\"] > scores[\"neg\"]:\n",
    "    print(\"Hey, It is a Postive Text\")\n",
    "else:\n",
    "    print(\"Hey, It is a Negative Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fe72be44-6052-4ab6-85b4-8181ba8834a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.128, 'neu': 0.714, 'pos': 0.158, 'compound': 1.0}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores(book)\n",
    "# book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f8818-39aa-45bd-a727-41be7bc171eb",
   "metadata": {},
   "source": [
    "# Find the sentiment analysis of the chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc9edba8-eb38-42fe-a16e-eeeea916706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(\"CHAPTER [0-9]+\")\n",
    "chapters = re.split(pattern, book)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1d53413-bb6e-4dd3-bc85-db8564a57714",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = chapters[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d0dbd10-e43f-4fa8-ad21-b4e1af72ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'neg': 0.128, 'neu': 0.714, 'pos': 0.158, 'compound': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for nr, chapter in enumerate(chapters):\n",
    "    scores = analyzer.polarity_scores(chapter)\n",
    "    print(nr + 1, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d12ab3b-107b-405f-b307-454d8ff14684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 : {'neg': 0.017, 'neu': 0.921, 'pos': 0.062, 'compound': 0.9731}\n",
      "Chapter 2 : {'neg': 0.108, 'neu': 0.757, 'pos': 0.135, 'compound': 0.9963}\n",
      "Chapter 3 : {'neg': 0.14, 'neu': 0.654, 'pos': 0.206, 'compound': 0.9999}\n",
      "Chapter 4 : {'neg': 0.116, 'neu': 0.703, 'pos': 0.181, 'compound': 0.9988}\n",
      "Chapter 5 : {'neg': 0.116, 'neu': 0.734, 'pos': 0.15, 'compound': 0.9998}\n"
     ]
    }
   ],
   "source": [
    "# Listing the Chapters\n",
    "for nr, chapter in enumerate(chapters, start=1):  # Start counting from 1\n",
    "    scores = analyzer.polarity_scores(chapter)\n",
    "    print(\"Chapter\", nr, \":\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a25b5c5c-785f-4921-99ad-f3ac20d1c65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 : {'neg': 0.017, 'neu': 0.921, 'pos': 0.062, 'compound': 0.9731}\n",
      "Chapter 2 : {'neg': 0.108, 'neu': 0.757, 'pos': 0.135, 'compound': 0.9963}\n",
      "Chapter 3 : {'neg': 0.14, 'neu': 0.654, 'pos': 0.206, 'compound': 0.9999}\n",
      "Chapter 4 : {'neg': 0.116, 'neu': 0.703, 'pos': 0.181, 'compound': 0.9988}\n",
      "Chapter 5 : {'neg': 0.116, 'neu': 0.734, 'pos': 0.15, 'compound': 0.9998}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Listing the Chapters\n",
    "    for nr, chapter in enumerate(chapters, start=1):  # Start counting from 1\n",
    "        scores = analyzer.polarity_scores(chapter)\n",
    "        print(\"Chapter\", nr, \":\", scores)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4932dc-8565-4bb5-b061-cdb9541321df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
